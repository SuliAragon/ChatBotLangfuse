from langchain_google_genai import ChatGoogleGenerativeAI
from .rag_engine import RAGEngine
from .pdf_generator import PDFGenerator
from .code_analyzer import CodeAnalyzer
import json
import os
from dotenv import load_dotenv
from langfuse.langchain import CallbackHandler
from langfuse import observe

load_dotenv()

# Inicializar componentes
pdf_gen = PDFGenerator()
analyzer = CodeAnalyzer()

# Intentar inicializar RAG, pero hacerlo opcional
rag_available = False
try:
    rag = RAGEngine()
    rag_available = True
except Exception as e:
    print(f"‚ö†Ô∏è RAG no disponible: {e}")
    rag = None


class DocumentationAgent:
    def __init__(self):
        # Inicializar Langfuse Callback Handler
        self.langfuse_handler = CallbackHandler()
        
        # Aumentar max_output_tokens para evitar cortes en JSON largos
        self.llm = ChatGoogleGenerativeAI(
            model="models/gemini-2.5-flash", 
            temperature=0.3,
            max_output_tokens=8192,
            callbacks=[self.langfuse_handler]
        )
    
    @observe(as_type="generation")
    def run(self, code):
        """Ejecuta el flujo de generaci√≥n de documentaci√≥n de forma secuencial."""
        try:
            print("\n" + "="*60)
            print("üöÄ Iniciando generaci√≥n de documentaci√≥n")
            print("="*60)
            
            # Paso 1: Analizar estructura del c√≥digo
            print("\nüìä Paso 1: Analizando estructura del c√≥digo...")
            # with langfuse_context.observe(name="Code Analysis", as_type="observation"):
            structure = analyzer.analyze(code, language="python")
            print(f"   ‚úì Encontradas {len(structure.get('functions', []))} funciones y {len(structure.get('classes', []))} clases")
            
            # Paso 2: Consultar mejores pr√°cticas (con fallback)
            print("\nüìö Paso 2: Consultando mejores pr√°cticas...")
            if rag_available and rag:
                try:
                    # with langfuse_context.observe(name="RAG Retrieval", as_type="observation"):
                    docs = rag.query("python documentation best practices")
                    best_practices = "\n".join([d.page_content for d in docs[:2]])  # Top 2
                    print("   ‚úì Mejores pr√°cticas obtenidas desde RAG")
                except Exception as e:
                    print(f"   ‚ö† RAG fall√≥, usando fallback: {e}")
                    best_practices = """- Usa docstrings en formato PEP 257
- Incluye type hints
- Documenta par√°metros y retornos"""
            else:
                best_practices = """- Usa docstrings en formato PEP 257
- Incluye type hints
- Documenta par√°metros y retornos"""
                print("   ‚úì Usando mejores pr√°cticas predefinidas")
            
            # Paso 3: Generar contenido con LLM
            print("\n‚úçÔ∏è Paso 3: Generando contenido de documentaci√≥n con IA...")
            prompt = f"""Eres un experto t√©cnico. Genera documentaci√≥n profesional para este c√≥digo.

C√ìDIGO ANALIZADO:
{json.dumps(structure, indent=2)}

MEJORES PR√ÅCTICAS A SEGUIR:
{best_practices}

INSTRUCCIONES:
1. Crea un t√≠tulo descriptivo
2. Para cada funci√≥n/clase, crea una secci√≥n con:
   - Heading con el nombre
   - P√°rrafo explicando su prop√≥sito
   - Si tiene docstring, incl√∫yela
3. Incluye una secci√≥n de mejores pr√°cticas aplicadas

FORMATO DE SALIDA (JSON):
Devuelve SOLO un JSON v√°lido con esta estructura exacta:
{{
  "title": "Documentaci√≥n de [nombre del c√≥digo]",
  "sections": [
    {{"type": "heading", "level": 1, "content": "Introducci√≥n"}},
    {{"type": "paragraph", "content": "Descripci√≥n general..."}},
    {{"type": "heading", "level": 2, "content": "Funci√≥n: nombre_funcion"}},
    {{"type": "paragraph", "content": "Descripci√≥n de la funci√≥n..."}},
    {{"type": "heading", "level": 1, "content": "Mejores Pr√°cticas Aplicadas"}},
    {{"type": "paragraph", "content": "Lista de mejores pr√°cticas..."}}
  ]
}}

IMPORTANTE: Devuelve SOLO el JSON, sin texto adicional antes o despu√©s. Aseg√∫rate de cerrar todas las llaves y comillas."""

            response = self.llm.invoke(prompt)
            content = response.content.strip()
            
            # Extracci√≥n robusta de JSON buscando el primer objeto JSON bien balanceado
            def extract_json(text: str) -> str:
                start = text.find('{')
                if start == -1:
                    return ''
                depth = 0
                in_string = False
                escape = False
                for i in range(start, len(text)):
                    ch = text[i]
                    if ch == '"' and not escape:
                        in_string = not in_string
                    if ch == '\\' and not escape:
                        escape = True
                        continue
                    else:
                        escape = False

                    if not in_string:
                        if ch == '{':
                            depth += 1
                        elif ch == '}':
                            depth -= 1
                            if depth == 0:
                                return text[start:i+1]
                return ''

            extracted = extract_json(content)
            if extracted:
                content = extracted
            else:
                # Fallback a regex (menos robusta) si no se encontr√≥ por balanceo
                import re
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    content = json_match.group(0)
            
            print(f"   ‚úì Contenido generado ({len(content)} caracteres)")
            
            # Paso 4: Generar PDF
            print("\nüìÑ Paso 4: Generando PDF...")
            # Intentar parsear el JSON extra√≠do
            try:
                data = json.loads(content)
            except json.JSONDecodeError as e:
                # Intentar pedir al LLM que corrija el JSON mal formado
                try:
                    repair_prompt = (
                        "El siguiente texto pretende ser un objeto JSON pero tiene errores de formato. "
                        "Devuelve SOLO el JSON v√°lido y corregido, sin ning√∫n comentario adicional.\n\n"
                        "TEXTO: \n" + content
                    )
                    repair_resp = self.llm.invoke(repair_prompt)
                    repaired = repair_resp.content.strip()
                    # Extraer de nuevo con el mismo m√©todo
                    extracted2 = extract_json(repaired)
                    if extracted2:
                        repaired = extracted2
                    data = json.loads(repaired)
                except Exception as e2:
                    raise json.JSONDecodeError(f"JSON repair failed: {e2}", doc=content, pos=0)
            
            # with langfuse_context.observe(name="PDF Generation", as_type="observation"):
            pdf_path = pdf_gen.generate(data)
            
            print("\n" + "="*60)
            print(f"‚úÖ √âXITO: Documentaci√≥n generada en {pdf_path}")
            print("="*60 + "\n")
            
            return {
                "output": f"Documentaci√≥n generada exitosamente en: {pdf_path}",
                "pdf_path": pdf_path
            }
            
        except json.JSONDecodeError as e:
            error_msg = f"Error al parsear JSON del LLM: {str(e)}\nContenido recibido (inicio): {content[:500]}..."
            print(f"\n‚ùå {error_msg}\n")
            return {"output": error_msg, "pdf_path": None}
        except Exception as e:
            error_msg = f"Error durante la generaci√≥n: {type(e).__name__}: {str(e)}"
            print(f"\n‚ùå {error_msg}\n")
            return {"output": error_msg, "pdf_path": None}

if __name__ == "__main__":
    # Test
    agent = DocumentationAgent()
    sample_code = """
    def suma(a, b):
        return a + b
    """
    print(agent.run(sample_code))
